{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce488115",
   "metadata": {},
   "source": [
    "# 1. LSTM 모델을 이용한 NLP Classification(스팸 메일 분류기)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c9b055",
   "metadata": {},
   "source": [
    "### LSTM 모델 사용하여 스팸 메일 분류 과정을 LSTM설계, 데이터 전처리 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77a6bce",
   "metadata": {},
   "source": [
    "#### 1.1 Fully Connceted Layer 복습\n",
    "#### - RNN과 LSTM 모델을 학습하기에 앞서 기본적인 ANN(Fully Connceted Layer)를 Python로 구성하는 것을 복습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "243bc29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07757c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self, num_output, input_size, hidden_size, device):\n",
    "        super(ANN, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.outlayer = nn.Linear(hidden_size, num_ouput)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.fc1(x).relu()\n",
    "        h = self.fc2(h).relu()\n",
    "        predict = self.outlayer(h)\n",
    "        return predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b180e5",
   "metadata": {},
   "source": [
    "### 1.2 LSTM for NLP\n",
    "#### - 가장 보편적으로 쓰이는 recurrent neural network 구조인 LSTM을 PyTorch로 꾸미는 과정. 기본적으로 텍스트를 다룰때에는 word2vec을 사용해도 되지만, nn.Embedding 레이어를 사용해 정수 인코딩 결과를 word2vec으로 만들어주는 레이어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39ca0e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_net(nn.Module):\n",
    "    def __init__(self, num_output, size_vocab, dim_embed, hidden_size, linear_size, num_layers, device):\n",
    "        super(LSTM_net, self).__init__()\n",
    "        self.device = device # GPU\n",
    "        self.num_output = num_output # 1\n",
    "        self.hidden_size = hidden_size # 128\n",
    "        self.num_layers = num_layers # 2\n",
    "        \n",
    "        self.embed = nn.Embedding(size_vocab, dim_embed)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = dim_embed, hidden_size = hidden_size,\n",
    "                           num_layers = num_layers, dropout = 0.3, bidirectional = True)\n",
    "        self.fclayer = nn.Linear(hidden_size, linear_size)\n",
    "        self.outlayer = nn.Linear(linear_size, num_output)\n",
    "        \n",
    "    def forward(self, x):   # x = 정수 인코딩 결과 [batch, seq_len]\n",
    "        scaler = 2 if self.lstm.bidirectinal == True else 1\n",
    "        \n",
    "        emb = self.embed(x) # word2vec 결과 [batch, seq_len, dim_embed]\n",
    "        \n",
    "        h_state = Variable(torch.zeros(self.num_layers*scaler, emb.size(0),\n",
    "                                      self.hidden_size, requlres_grad = True)).to(self.device)\n",
    "        c_state = Variable(torch.zeros(self.num_layers*scaler, emb.size(0),\n",
    "                                      self.hidden_size, requlres_grad = True)).to(self.device)\n",
    "        lstm_out, (h,c) = self.lstm(emb.transpose(1,0), (h_state, c_state))\n",
    "        # 실제로 lstm   # (h,c) = (hidden, cell)\n",
    "        # emb : [seq_len, batch, dim_embed], 1번째 2번째 차원을 바꾸기 위해 transpose넣어줌\n",
    "        # seq_len가 가장 앞으로 오게 만들어야 함\n",
    "        \n",
    "        h = h[-1] # important # 마지막 타임의 hidden만 가져오겠다 # h = [batch, hidden]\n",
    "        h = self.fclayer(h).relu()\n",
    "        predict = self.outlayer(h)  # predict = [batch, num_output]\n",
    "        return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e71c46",
   "metadata": {},
   "source": [
    "### 1.3 Spam Mail Classification : 데이터 전처리\n",
    "####  스팸 메일을 분류할 수 있는 이진 분류기를 LSTM을 이용해 꾸며보는 예시. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ae383d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32db77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('')\n",
    "display(data.info(), data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688e4d68",
   "metadata": {},
   "source": [
    "- 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "986736a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "token_text = []\n",
    "for i in range(5728):\n",
    "    token = word_tokenize(data.iloc[i,0])\n",
    "    token_stop_text = []\n",
    "    for w in token:\n",
    "        if w not in stop_words:\n",
    "            token_stop_text.append(w)\n",
    "    token_text.append(token_stop_text)\n",
    "    \n",
    "print('After cleaning :', len(token_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9648fd0",
   "metadata": {},
   "source": [
    "- 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db1f4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(token_text)\n",
    "print(len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecd5bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoded = tokenizer.texts_to_sequences(token_text)\n",
    "print(text_encoded[0]) # 첫번째 메일의 정수 인코딩 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f5894c",
   "metadata": {},
   "source": [
    "- 학습을 위한 Label : Spam인 경우 1, Normal Text인 경우 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e4d4d",
   "metadata": {},
   "source": [
    "- Padding 및 데이터 자르기\n",
    "-- 이메일은 보통 다수의 문장으로 이루어져 있기 때문에 정제 및 추출을 거치더라도 1개 샘플의 길이가 길 수 있습니다. 따라서 maxlen을 설정하여 maxlen 이하의 토큰을 가진 이메일은 padding을, maxlen 이상의 토큰을 가진 이메일은 첫 100개만 사용하고 나머지는 버림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1366ba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(text_encoded))\n",
    "print(np.shape(text_label))\n",
    "maxlen = 0\n",
    "for w in text_encoded:\n",
    "    if len(w) >= maxlen:\n",
    "        maxlen = len(w)\n",
    "print(maxlen)\n",
    "\n",
    "maxlen = 100\n",
    "rowdata = []\n",
    "for w in text_encoded:\n",
    "    if len(w) >= maxlen:\n",
    "        rowdata.append(x[:maxlen])\n",
    "    else:\n",
    "        rowdata.append(np.pad(w, (0,maxlen), 'constant', constant_values=0)[:maxlen])\n",
    "text_padded = np.concatenate(rowdata, axis=0).reshape(-1, maxlen)\n",
    "print(np.shape(text_padded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9312fc6a",
   "metadata": {},
   "source": [
    "### 1.4 학습을 위한 Dataset 만들기 및 학습 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd0f918",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "import torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch import LongTensor as LT\n",
    "from torch import FloatTensor as FT\n",
    "\n",
    "class Generate_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, xdata, ydata, device):\n",
    "        self.x_data = xdata\n",
    "        self.y_data = ydata\n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    def __getitem__(self, idx):\n",
    "        x = LT(self.x_data[idx]).to(self.device)\n",
    "        y = LT(self.x_data[idx]).to(self.device)   \n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5510dee4",
   "metadata": {},
   "source": [
    "- Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6f0a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasett = Generate_Dataset(text_padded[:5000,:].reshape([-1,1]), device)\n",
    "trainset, testset = random_split(dataset, [4500,500])\n",
    "train_loader = DataLoader(trainset, batch_size = 256, shuffle =True)\n",
    "test_loader = DataLoader(testset, batch_size = 500, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9f0797",
   "metadata": {},
   "source": [
    "- Define Network and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783ec7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_net = LSTM_net(num_output = 2, size_vocab = len(tokenizer, word_index), dim_embed = 64,\n",
    "                   hidden_size = 64, linear_size = 64, num_layers = 1, device = device)\n",
    "optimizer = torch.optim.Adam(lstm_net.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54126d3c",
   "metadata": {},
   "source": [
    "- Training Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c61f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in range(10):\n",
    "    print('Epoch', epoch)\n",
    "    with tqdm(train_loader, unit = 'batch') as tepoch:\n",
    "        for x,y in tepoch:\n",
    "            predict = lstm_net(x) # x = email word --> predict = [batch, 2]\n",
    "            loss = torch.nn.functional.cross_entropy(predict, y.ravel())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(loss)\n",
    "#             print(loss)\n",
    "#             tepoch.set_description(f\"Epoch {epoch}\")\n",
    "#             tepoch.set_postrix(loss = loss.item())\n",
    "\n",
    "# loss = (predict 자리에 원핫 인코딩 결과가 들어가야함)\n",
    "# y = [batch, 1] 에서 1(차원)이 사라지고 batch 차원으로 만들어야 하기 때문에 ravel 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff7c57f",
   "metadata": {},
   "source": [
    "- Test the Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd7ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(test_loader, unit='batch') as tepoch:\n",
    "    for x, y in tepoch:\n",
    "        predict = lstm_net(x).argmax(1).detach().numpy()\n",
    "        answer = y.revel().detach().numpy()\n",
    "score = 0\n",
    "for i in range(len(predict)):\n",
    "    if predict[i] == answer[i]:\n",
    "        score += 1\n",
    "print(score, 'out of 500, accuracy is', score/500*100, '%')\n",
    "\n",
    "# 500개 테스트 해서 처음 predict = [500, 2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48302c6",
   "metadata": {},
   "source": [
    "## NLP 문제 해결\n",
    "### 1. Dataset 전처리\n",
    "#### - token -> 정제/추출 -(이 과정에서 padding 진행)-> 정수인코딩\n",
    "       padding :  기계번역시 사용 x, 분류문제시 사용 o\n",
    "                  미니배치 사이즈 똑같이 맞추기 위해서 padding\n",
    "### 2. Netword 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b5e2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d44c51d0",
   "metadata": {},
   "source": [
    "# 2. seq2seq 모델 이용한 NLP machine translation\n",
    "### LSTM 모델 이용한 seq2seq 모델에서 기계 번역 구현 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e65ed7",
   "metadata": {},
   "source": [
    "### 2.1 Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2507e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "os.system(\"python -m spacy download en_core_web_sm\")\n",
    "os.system(\"python -m spacy download de_core_news_sm\")\n",
    "\n",
    "# Source from [1]\n",
    "spacy_german = spacy.load('de_core_news_sm')\n",
    "spacy_english = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ce76f4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Field' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_en\u001b[39m(text):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [tok\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m spacy_english\u001b[38;5;241m.\u001b[39mtokenizer(text)]\n\u001b[1;32m----> 5\u001b[0m SRC \u001b[38;5;241m=\u001b[39m \u001b[43mField\u001b[49m(tokenize \u001b[38;5;241m=\u001b[39m tokenize_de, init_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<sos>\u001b[39m\u001b[38;5;124m'\u001b[39m, eos_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<eos>\u001b[39m\u001b[38;5;124m'\u001b[39m, lower \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m TRG \u001b[38;5;241m=\u001b[39m Field(tokenize \u001b[38;5;241m=\u001b[39m tokenize_en, init_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<sos>\u001b[39m\u001b[38;5;124m'\u001b[39m, eos_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<eos>\u001b[39m\u001b[38;5;124m'\u001b[39m, lower \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Field' is not defined"
     ]
    }
   ],
   "source": [
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_german.tokenizer(text)][::-1]\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_english.tokenizer(text)]\n",
    "SRC = Field(tokenize = tokenize_de, init_token = '<sos>', eos_token = '<eos>', lower = True)\n",
    "TRG = Field(tokenize = tokenize_en, init_token = '<sos>', eos_token = '<eos>', lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25046f7d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext.legacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Multi30k\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Field\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchtext.legacy'"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cabb34",
   "metadata": {},
   "source": [
    "### 2.2 Network Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de586f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d15c4fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq_Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, dim_embed, hidden_size, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size \n",
    "        self.num_layers = num_layers \n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, dim_embed)\n",
    "        self.lstm = nn.LSTM(dim_embed, hidden_size, num_layers, dropout = dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        outputs, (hidden, cell) = self.lstm(self.dropout(self.embed(src)))\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91f00bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq_Decoder(nn.Module):\n",
    "    def __init__(self, output_size, dim_embed, hidden_size, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size \n",
    "        self.hidden_size = hidden_size \n",
    "        self.num_layers = num_layers \n",
    "        \n",
    "        self.embed = nn.Embedding(output_size, dim_embed)\n",
    "        self.lstm = nn.LSTM(dim_embed, hidden_size, num_layers, dropout = dropout)\n",
    "        self.fclayer = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,input_data, hidden, cell):\n",
    "        input_data = input_data.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embed(input_data))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded,(hidden, cell))\n",
    "        prediction = self.fclayer(output.squeeze(0))\n",
    "        return (prediction, hidden, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9e82317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "    class seq2seq(nn.Module):\n",
    "        def __init__(self, encoder, decoder, device):\n",
    "            super().__init__()\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "            self.device = device\n",
    "\n",
    "        def forward(self, source, target, tf_ratio = 0.5):\n",
    "            batch_size = target.shape[1]\n",
    "            translation_length = target.shape[0]\n",
    "            target_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        outputs = torch.zeros(translation_length, batch_size, target_vocab_size).to(self.device)\n",
    "        hidden, cell = self.encoder(source)\n",
    "        input_trans = target[0,:]\n",
    "        \n",
    "        for t in range(1, translation_length):\n",
    "            output, hidden, cell = self.decoder(input_trans, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < tf_ratio\n",
    "            input_trans = target[t] if teacher_force else output.argmax(1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b015cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9744\\2492880700.py:1: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xe (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SRC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m enc \u001b[38;5;241m=\u001b[39m seq_Encoder(\u001b[38;5;28mlen\u001b[39m(\u001b[43mSRC\u001b[49m\u001b[38;5;241m.\u001b[39mvocab), \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.3\u001b[39m)\n\u001b[0;32m      3\u001b[0m dec \u001b[38;5;241m=\u001b[39m seq_Decoder(\u001b[38;5;28mlen\u001b[39m(TRG\u001b[38;5;241m.\u001b[39mvocab), \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.3\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SRC' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "enc = seq_Encoder(len(SRC.vocab), 64, 64, 1, 0.3)\n",
    "dec = seq_Decoder(len(TRG.vocab), 64, 64, 1, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20225070",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_net = seq2seq(enc, dec, device).to(device)\n",
    "optimizer = torch.optim.Adam(seq_net.parameters(), lr =0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708873c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.lagacy.data import Bucketlterator\n",
    "train_iterator, valid_iterator, test_iterator = Bucketlterator.splits((train_data, vaild_data, test_data),\n",
    "                                                                     batch_size=256, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c61c6f3",
   "metadata": {},
   "source": [
    "### 2.3 Train the Translator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e8d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_index = TRG.vocab.stoi[TRG.pad_token] # padding이 일어나는 토큰\n",
    "lossfcn = nn.CrossEntropyLoss(ignore_index = pad_index)\n",
    "\n",
    "for epoch in range(10):\n",
    "    loss_epoch = 0\n",
    "    for batch in train_iterator:\n",
    "        source_data = batch.src\n",
    "        target_data = batch.trg\n",
    "        target_pred = seq_net(source_data,target_data)\n",
    "        target_pred = target_pred[1:].view(-1, target_pred.shape[-1])\n",
    "        target_data = target_Data[:1].view(-1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = lossfcn(target_pred, target_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch += loss.item()\n",
    "    print(\"Epoch\", epoch, 'Loss', loss_epoch/len(train_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777c2308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0463f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afcf89f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
