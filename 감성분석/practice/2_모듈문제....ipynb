{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "113032df",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_helper'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_helper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_vocab_to_dict, build_fasttext, dataset_iterator, load_verb_count\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'data_helper'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from data_helper import read_vocab_to_dict, build_fasttext, dataset_iterator, load_verb_count\n",
    "from model import Model\n",
    "import pickle\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "    # read word and verb dict\n",
    "    print(\"load dictionary...\")\n",
    "    example_dict = dict()\n",
    "    with open('vocabulary/example_dict.pkl', 'rb') as f:\n",
    "        example_dict = pickle.load(f)\n",
    "    word_dict = read_vocab_to_dict(\"vocabulary/word_vocab.txt\")\n",
    "    verb_dict = read_vocab_to_dict(\"vocabulary/verb_vocab.txt\")\n",
    "    verb_vocab_count = load_verb_count(\"vocabulary/verb_count.txt\")\n",
    "\n",
    "    flags = tf.flags\n",
    "    flags.DEFINE_integer(\"neg_sample\", 10, \"number of negative samples\")\n",
    "    flags.DEFINE_integer(\"word_dim\", 300, \"word embedding dimension\")\n",
    "    flags.DEFINE_integer(\"num_units\", 100, \"number of units for rnn cell and hidden layer of ffn\")\n",
    "    flags.DEFINE_integer(\"output_units\", 200, \"number of units for output part\")\n",
    "    flags.DEFINE_bool(\"use_pretrained\", True, \"use pretrained word2vec\")\n",
    "    flags.DEFINE_bool(\"tune_emb\", True, \"tune pretrained embeddings while training\")\n",
    "    flags.DEFINE_string(\"pretrained_context\", \"embeddings/context_embeddings.npz\", \"pretrained context embedding path\")\n",
    "    flags.DEFINE_string(\"pretrained_target\", \"embeddings/target_embeddings.npz\", \"pretrained target embedding path\")\n",
    "    flags.DEFINE_integer(\"vocab_size\", len(word_dict), \"word vocab size\")\n",
    "    flags.DEFINE_integer(\"verb_size\", len(verb_dict), \"verb vocab size\")\n",
    "    flags.DEFINE_float(\"lr\", 0.001, \"learning_rate\")\n",
    "    flags.DEFINE_integer(\"batch_size\", 300, \"batch size\")\n",
    "    flags.DEFINE_integer(\"epochs\", 3, \"epochs\")\n",
    "    flags.DEFINE_string(\"ckpt\", \"ckpt/\", \"checkpoint path\")\n",
    "    flags.DEFINE_string(\"model_name\", \"train_concept\", \"model name\")\n",
    "    config = flags.FLAGS\n",
    "\n",
    "\n",
    "    # initialize with pretrained fasttext embeddings\n",
    "    if not os.path.exists(config.pretrained_context) or not os.path.exists(config.pretrained_target):\n",
    "        build_fasttext(config.fasttext_path, config.pretrained_context, config.pretrained_target, word_dict, verb_dict, config.word_dim)\n",
    "\n",
    "    if not os.path.exists(config.ckpt):\n",
    "        os.makedirs(config.ckpt)\n",
    "\n",
    "    # training the model\n",
    "    print(\"start training...\")\n",
    "\n",
    "    sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    sess_config.gpu_options.allow_growth = True\n",
    "\n",
    "    with tf.Session(config=sess_config) as sess:\n",
    "        # build model\n",
    "        print(\"build model...\")\n",
    "        model = Model(config, verb_vocab_count)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver(max_to_keep=1)\n",
    "        for epoch in range(config.epochs):\n",
    "            print('epoch :', epoch)\n",
    "            for i, data in enumerate(dataset_iterator(example_dict, word_dict, verb_dict, config.batch_size)):\n",
    "                feed_dict = model.get_feed_dict(data, is_train=True, lr=config.lr)\n",
    "                _, losses = sess.run([model.train_op, model.loss], feed_dict=feed_dict)\n",
    "        # save the model\n",
    "        saver.save(sess, config.ckpt + config.model_name, global_step=config.epochs)\n",
    "        # save the trained target embedding\n",
    "        target_emb = sess.run(model.verb_embeddings)\n",
    "        np.savez_compressed(\"embeddings/trained_target_emb.npz\", embeddings=target_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6024c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: senticnet in c:\\users\\admin\\anaconda3\\envs\\nltk\\lib\\site-packages (1.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install senticnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62912b2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordfreq in c:\\users\\admin\\anaconda3\\envs\\nltk\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: ftfy>=6.1 in c:\\users\\admin\\anaconda3\\envs\\nltk\\lib\\site-packages (from wordfreq) (6.1.1)\n",
      "Requirement already satisfied: regex>=2020.04.04 in c:\\users\\admin\\anaconda3\\envs\\nltk\\lib\\site-packages (from wordfreq) (2022.8.17)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\admin\\anaconda3\\envs\\nltk\\lib\\site-packages (from wordfreq) (1.0.4)\n",
      "Requirement already satisfied: langcodes>=3.0 in c:\\users\\admin\\anaconda3\\envs\\nltk\\lib\\site-packages (from wordfreq) (3.3.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\admin\\anaconda3\\envs\\nltk\\lib\\site-packages (from ftfy>=6.1->wordfreq) (0.2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ad7c155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from senticnet import senticnet\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "# from wordfreq import word_frequency\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8eb5821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### basic functions #####\n",
    "class Hypernyms:\n",
    "    def __init__(self, word, synset_obj):\n",
    "        self.level = [[],[],[],[]]\n",
    "        self.level[0] = [lemma.name().lower() for lemma in synset_obj.lemmas()]\n",
    "        self.level[0].remove(word)\n",
    "        l1, l2, l3 = [], [], []\n",
    "        l1 = synset_obj.hypernyms()\n",
    "        for hyper in l1:\n",
    "            self.level[1] += [lemma.name().lower() for lemma in hyper.lemmas()]\n",
    "            l2 += hyper.hypernyms()\n",
    "        for hyper in l2:\n",
    "            self.level[2] += [lemma.name().lower() for lemma in hyper.lemmas()]\n",
    "            l3 += hyper.hypernyms()\n",
    "        for hyper in l3:\n",
    "            self.level[3] += [lemma.name().lower() for lemma in hyper.lemmas()]\n",
    "\n",
    "def weighted_sentic(sv1, fr1, sv2, fr2):\n",
    "    #input: sentic values and their frequencies. 0 <= frequency <= 1\n",
    "    #output: weighted sum of sentic values\n",
    "\n",
    "    #polarity values must be equal\n",
    "    polarity = sv1[6]\n",
    "\n",
    "    tf = fr1+fr2\n",
    "    fr1 /= tf\n",
    "    fr2 /= tf\n",
    "\n",
    "    pl = float(sv1[0]) * fr1 + float(sv2[0]) * fr2\n",
    "    at = float(sv1[1]) * fr1 + float(sv2[1]) * fr2\n",
    "    se = float(sv1[2]) * fr1 + float(sv2[2]) * fr2\n",
    "    ap = float(sv1[3]) * fr1 + float(sv2[3]) * fr2\n",
    "    pv = float(sv1[7]) * fr1 + float(sv2[7]) * fr2\n",
    "\n",
    "    sentic = [pl, at, se, ap]\n",
    "\n",
    "    return [round(pl, 3), round(at, 3), round(se, 3), round(ap, 3)] + sentic_to_sentiments(sentic) + [polarity, round(pv, 3)]\n",
    "\n",
    "def sentic_to_sentiments(sentic):\n",
    "    #input: sentic values only [0.1, 0.2, 0.3, 0.4]\n",
    "    #output: sentiments #joy, #admiration\n",
    "\n",
    "    sentiment_dict = {\n",
    "            # col 1  or   col 2\n",
    "        0: ['#sadness', '#joy'],\n",
    "        1: ['#surprise', '#interest'],\n",
    "        2: ['#fear', '#anger'],\n",
    "        3: ['#disgust', '#admiration']\n",
    "    }\n",
    "\n",
    "    first_sentic, second_sentic = np.argsort(np.absolute(sentic))[-2:][::-1]\n",
    "    col1 = math.floor(sentic[first_sentic]) + 1\n",
    "    col2 = math.floor(sentic[second_sentic]) + 1\n",
    "\n",
    "    return[sentiment_dict[first_sentic][col1], sentiment_dict[second_sentic][col2]]\n",
    "\n",
    "def sentic_to_string(sentic_value):\n",
    "    #input: sentic value\n",
    "    #output: its string it is for recording in txt file\n",
    "    string = ''\n",
    "    for s in sentic_value:\n",
    "        string += '\\''+str(s)+'\\', '\n",
    "    return string[:-2]\n",
    "\n",
    "def load_korean_wordnet_offset(kw_path):\n",
    "    kw_offset = []\n",
    "    with open(kw_path, 'r', encoding='utf8') as f:\n",
    "        for line in f.read().split('\\n')[1:]:\n",
    "            kw_offset.append(line.split('\\t')[0])\n",
    "    return kw_offset\n",
    "\n",
    "def load_similarities(trained_emb_path):\n",
    "    trained_target_emb = np.load(open(trained_emb_path, 'rb'))['embeddings']\n",
    "    _sparse = sparse.csr_matrix(trained_target_emb)\n",
    "    similarities = cosine_similarity(_sparse)\n",
    "    return similarities\n",
    "\n",
    "def read_vocab_to_dict(filename):\n",
    "    vocab = dict()\n",
    "    with open(filename, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        for idx, word in enumerate(f):\n",
    "            word = word.lstrip().rstrip()\n",
    "            vocab[word] = idx\n",
    "    #return word_dict\n",
    "    return vocab\n",
    "\n",
    "def index_to_word(word_dict):\n",
    "    idx_word = dict()\n",
    "    for word, idx in word_dict.items():\n",
    "        idx_word[idx] = word\n",
    "    return idx_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50eca7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### direct mapping #####\n",
    "\n",
    "def direct_mapping(korean_wordnet_path):\n",
    "    print('Direct mapping started...')\n",
    "    # input: korean_wordnet_path - 00018158-v\tVerb\tuprise, rise, arise, turn_out, get_up\t기상하\n",
    "    # output: offset_sentic_dict[\"00044455-n\"] = ['emergence', '#joy', '#surprise', 'positive', '0.726', 'appearance', 'start', 'casus_belli', 'beginning', 'egress']\n",
    "\n",
    "    # step 1: make word - synset dict\n",
    "    word_synset_dict = dict()\n",
    "    with open(korean_wordnet_path, 'r', encoding='utf8') as f:\n",
    "        lines = f.read().split('\\n')[1:]\n",
    "        for line in lines:\n",
    "            words = line.split('\\t')[2].replace(' ','').split(',')\n",
    "            offset = line.split('\\t')[0]\n",
    "            for word in words:\n",
    "                if word not in word_synset_dict:\n",
    "                    word_synset_dict[word] = [offset]\n",
    "                else:\n",
    "                    word_synset_dict[word].append(offset)\n",
    "\n",
    "\n",
    "    # step 2: make synset - sentic dict\n",
    "    with open('vocabulary/affectnet_dict.pkl', 'rb') as f:\n",
    "        aff_fr_dict = pickle.load(f)\n",
    "    fr_offset_dict = dict()\n",
    "    offset_sentic_dict = dict()\n",
    "    deleted_offset = []\n",
    "    for word in word_synset_dict:\n",
    "        if word not in senticnet:\n",
    "            continue\n",
    "\n",
    "        found = False\n",
    "        synsets = [wn.synset_from_pos_and_offset(synset_id[-1], int(synset_id[:8].lstrip('0'))) for synset_id in word_synset_dict[word]]\n",
    "        num_synsets = len(synsets)\n",
    "        hypernyms = [Hypernyms(word, synset) for synset in synsets]\n",
    "        for i in range(4):\n",
    "            for j in range(num_synsets):\n",
    "                offset = word_synset_dict[word][j]\n",
    "                if offset in deleted_offset:\n",
    "                    continue\n",
    "\n",
    "                for hypernym in hypernyms[j].level[i]:\n",
    "                    if hypernym not in senticnet[word]:\n",
    "                        continue\n",
    "\n",
    "                    # hypernym is in semantics so, the synset could represent the meaning of the word\n",
    "                    if offset not in offset_sentic_dict:\n",
    "                        offset_sentic_dict[offset] = senticnet[word]\n",
    "                        fr_offset_dict[offset] = aff_fr_dict[word]\n",
    "                    else:\n",
    "                        # if polarity is different, deleted existing offset_sentic pair.\n",
    "                        if offset_sentic_dict[offset][6] != senticnet[word][6]:\n",
    "                            del offset_sentic_dict[offset]\n",
    "                            deleted_offset.append(offset)\n",
    "                            #the offset is not allowed, so we no longer need to see other hypernyms\n",
    "                            break\n",
    "\n",
    "                        offset_sentic_dict[offset] = weighted_sentic(offset_sentic_dict[offset], fr_offset_dict[offset], senticnet[word], aff_fr_dict[word])\n",
    "                        fr_offset_dict[offset] += aff_fr_dict[word]\n",
    "                    found = True\n",
    "                    break\n",
    "            # no deeper progress\n",
    "            if found:\n",
    "                # lesk algorithm is applied to those which were not mapped directly ( for later use )\n",
    "                del senticnet[word]\n",
    "                break\n",
    "\n",
    "    return offset_sentic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d490a9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### lesk algorithm #####\n",
    "\n",
    "def apply_lesk(offset_sentic_dict):\n",
    "    print('Appling Lesk Algorithms Started...')\n",
    "    # making another offset_sentic_dict for rest of concepts and concat it to the existed one.\n",
    "    # input: offset_sentic_dict[\"00044455-n\"] = ['0.1', '0.1', '0.1', '0.1', #joy', '#surprise', 'positive', '0.726', 'appearance', 'start', 'casus_belli', 'beginning', 'egress'] // semantics might not included\n",
    "    # output: last_offset_sentic_dict[\"00044455-n\"] = ['0.1', '0.1', '0.1', '0.1', '#joy', '#surprise', 'positive', '0.726', 'appearance', 'start', 'casus_belli', 'beginning', 'egress']\n",
    "\n",
    "    with open('vocabulary/affectnet_dict.pkl', 'rb') as f:\n",
    "        aff_fr_dict = pickle.load(f)\n",
    "    fr_offset_dict = dict()\n",
    "\n",
    "    deleted_offset = []\n",
    "\n",
    "    #direct mapped words were deleted before\n",
    "    for word, value in senticnet.items():\n",
    "        context = word\n",
    "        found = False\n",
    "        for i in range(8, 13):\n",
    "            context += senticnet[word][i] + ' '\n",
    "        try:\n",
    "            synset = lesk(context, word)\n",
    "            offset = str(synset.offset()).zfill(8) + '-' + synset.pos()\n",
    "            found = True\n",
    "        except AttributeError:\n",
    "            # not found\n",
    "            # sequentially, because it is arranged by c. similarity\n",
    "            for v in value[8:13]:\n",
    "                try:\n",
    "                    synset = lesk(context, v)\n",
    "                    offset = str(synset.offset()).zfill(8) + '-' + synset.pos()\n",
    "                    found = True\n",
    "                except AttributeError:\n",
    "                    continue\n",
    "        if found == False:\n",
    "            continue\n",
    "        # Direct mapped offset is not considered\n",
    "        if offset in offset_sentic_dict:\n",
    "            continue\n",
    "        if offset in deleted_offset:\n",
    "            continue\n",
    "\n",
    "        if offset not in offset_sentic_dict:\n",
    "            offset_sentic_dict[offset] = value\n",
    "            fr_offset_dict[offset] = aff_fr_dict[word]\n",
    "        else:\n",
    "            if offset_sentic_dict[offset][6] != value[6]:\n",
    "                del offset_sentic_dict[offset]\n",
    "                deleted_offset.append(offset)\n",
    "                continue\n",
    "            else:\n",
    "                offset_sentic_dict[offset] = weighted_sentic(offset_sentic_dict[offset], fr_offset_dict[offset], value, aff_fr_dict[word])\n",
    "                fr_offset_dict[offset] += aff_fr_dict[word]\n",
    "\n",
    "    return offset_sentic_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28a4b121",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'embeddings/trained_target_emb.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m ksenticnet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m      6\u001b[0m similarity_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()    \u001b[38;5;66;03m# to store max similarity on each synset for a korean word\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m similarity_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mload_similarities\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membeddings/trained_target_emb.npz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m word_idx \u001b[38;5;241m=\u001b[39m read_vocab_to_dict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvocabulary/verb_vocab.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m idx_word \u001b[38;5;241m=\u001b[39m index_to_word(word_idx)\n",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36mload_similarities\u001b[1;34m(trained_emb_path)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_similarities\u001b[39m(trained_emb_path):\n\u001b[1;32m---> 73\u001b[0m     trained_target_emb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrained_emb_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     74\u001b[0m     _sparse \u001b[38;5;241m=\u001b[39m sparse\u001b[38;5;241m.\u001b[39mcsr_matrix(trained_target_emb)\n\u001b[0;32m     75\u001b[0m     similarities \u001b[38;5;241m=\u001b[39m cosine_similarity(_sparse)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'embeddings/trained_target_emb.npz'"
     ]
    }
   ],
   "source": [
    "##### main #####\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    ksenticnet = dict()\n",
    "    similarity_dict = dict()    # to store max similarity on each synset for a korean word\n",
    "    similarity_matrix = load_similarities('embeddings/trained_target_emb.npz')\n",
    "    word_idx = read_vocab_to_dict('vocabulary/verb_vocab.txt')\n",
    "    idx_word = index_to_word(word_idx)\n",
    "    korean_wordnet_path = 'kwn_1.0/kwn_synset_list.tsv'\n",
    "\n",
    "    offset_sentic_dict = direct_mapping(korean_wordnet_path)\n",
    "    offset_sentic_dict = apply_lesk(offset_sentic_dict)\n",
    "\n",
    "    fr_offset_dict = dict()\n",
    "    fr_ko_dict = dict()\n",
    "\n",
    "    deleted_kor = []\n",
    "\n",
    "    lines = open(korean_wordnet_path, 'r', encoding='utf8').read().split('\\n')[1:]\n",
    "\n",
    "    print('Making KSenticnet Started...')\n",
    "    for line in lines:\n",
    "        offset = line.split('\\t')[0]\n",
    "\n",
    "        if offset not in offset_sentic_dict:\n",
    "            continue\n",
    "\n",
    "        en_words = line.split('\\t')[2].replace(' ','').split(',')\n",
    "        kor_words = line.split('\\t')[3].replace(' ', '').split(',')\n",
    "\n",
    "        offset_freq = np.sum([word_frequency(en_word.replace('_', ' '), 'en') for en_word in en_words])\n",
    "        fr_offset_dict[offset] = offset_freq\n",
    "\n",
    "        avg_similarity = lambda kor, kors: np.sum([similarity_matrix[word_idx[kor]][word_idx[x]] for x in kors]) / len(kors)\n",
    "\n",
    "        for kor_word in kor_words:\n",
    "            if kor_word in deleted_kor:\n",
    "                continue\n",
    "            ksemantics = kor_words[:]\n",
    "            ksemantics.remove(kor_word)\n",
    "            if kor_word not in ksenticnet:\n",
    "                #only one korean word in one synset\n",
    "                if len(kor_words) == 1:\n",
    "                    similarity_dict[kor_word] = 1\n",
    "                    fr_ko_dict[kor_word] = fr_offset_dict[offset]\n",
    "                else:\n",
    "                    similarity_dict[kor_word] = avg_similarity(kor_word, ksemantics)\n",
    "                ksenticnet[kor_word] = offset_sentic_dict[offset][:8] + ksemantics\n",
    "            else:\n",
    "                if len(kor_words) == 1:\n",
    "                    # existed one was not the only word for synset. Which means that this one has to represent the value.\n",
    "                    if similarity_dict[kor_word] != 1:\n",
    "                        similarity_dict[kor_word] = 1\n",
    "                        fr_ko_dict[kor_word] = fr_offset_dict[offset]\n",
    "                        ksenticnet[kor_word] = offset_sentic_dict[offset][:8]\n",
    "                    else:\n",
    "                        if ksenticnet[kor_word][6] != offset_sentic_dict[offset][6]:\n",
    "                            del ksenticnet[kor_word]\n",
    "                            deleted_kor.append(kor_word)\n",
    "                            continue\n",
    "                        fr_ko_dict[kor_word] += fr_offset_dict[offset]\n",
    "                        # to prevent both frequencies are 0\n",
    "                        ksenticnet[kor_word] = weighted_sentic(ksenticnet[kor_word], max(fr_ko_dict[kor_word], 1), offset_sentic_dict[offset], max(fr_offset_dict[offset], 1))\n",
    "                #we have to compare each synset's similarity on the korean word and choose only one.\n",
    "                else:\n",
    "                    avg_simil = avg_similarity(kor_word, ksemantics)\n",
    "                    if avg_simil > similarity_dict[kor_word]:\n",
    "                        ksenticnet[kor_word] = offset_sentic_dict[offset][:8] + ksemantics\n",
    "                        similarity_dict[kor_word] = avg_simil\n",
    "\n",
    "    for k, v in ksenticnet.items():\n",
    "        idx = [i for i, x in enumerate(v[8:]) if x not in ksenticnet.keys()]\n",
    "        for i in idx[::-1]:\n",
    "            del ksenticnet[k][i+8]\n",
    "    ksenticnet = {key: ksenticnet[key] for key in sorted(ksenticnet)}\n",
    "\n",
    "    ksenticnet_file = open('ksenticnet_kaist.py', 'w', encoding='utf8')\n",
    "    ksenticnet_file.write('ksenticnet = {}\\n')\n",
    "    for key, value in ksenticnet.items():\n",
    "        ksenticnet_file.write('ksenticnet[\\\"' + key + '\\\"] = [' + sentic_to_string(value) + ']\\n')\n",
    "\n",
    "    ksenticnet_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7e9a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0c5b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ca6798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ab4748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f96471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
